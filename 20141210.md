# Thoughts on Programming Languages (Part 2 — RAII)

## Block Scoping with RAII

I only recently came across some the term RAII, which stands for “Resource
Acquisition is Initialization.”
[Someone on StackOverflow](http://stackoverflow.com/questions/2321511) called it
“a really terrible name,” and I agree to some extent that it’s definitely not
the best name for such a nice concept.
The biggest reason is that the phrase only captures the initialization sense of
things and excludes the termination part.
But when we discuss RAII, a great emphasis is placed on termination, probably
more so than on initialization.

## RAII is a Semantic and Syntactical Solution

I think RAII is a great aid in systems languages.
A popular example of RAII is smart pointers managing heap memory.
But heap memory isn’t the only type of resource we need to care about — if we
want to stick to the RAII notion, we need to apply it to any type of resource
such as files, DB connections, etc.
I think that the key takeaway of the RAII idiom is this: **the semantic access
scope of an identifier must equal the lifetime of the corresponding resource**.
I’d like to praise this philosophy for two reasons.

-   It helps the programmer in case of forgetfulness.

-   This reason is more of a philosophical nature: this equality nicely
abstracts the concept of physical resource management away into a linguistic,
semantic level.
It helps us think the semantic way.
After all, programming langauges are human things — we’re trying to generate
meaning by handling hardware.
I think RAII is one of those signs showing that software engineers have really
tried toward good abstraction over hardware.

## Block scoping is nice

Note that I’m praising two things here — not only the clever use of destructors,
but the general idea of block scoping.
I mean, block scoping may not be the best sort of scoping, but it helps bound
variables to my myopic memory, helps me recycle variable names, and thus helps
keep my code organized.
I think it’s a nice thing to have.

## RAII under Exceptions

What if an exception occurs within a block in which a resource may be leaked?
You might have seen the good old Java solution (I don’t know if the
try-with-resource clause has changed things):

```Java
// borrowed from docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html

static String readFirstLineFromFileWithFinallyBlock(String path) throws IOException {
  BufferedReader br = new BufferedReader(new FileReader(path));
  try {
    return br.readLine();
  } finally {
    if (br != null) br.close();
  }
}
```

What if that try statement was super long?
The finally part might get little attention, so little that it may get
forgotten.

RAII may resolve this issue.
Assume a C++ equivalent of that method, and say an IOException occurred within
it.
If the BufferedReader was declared in an RAII style, then we wouldn’t need such
a `finally` block at the end, because in case of an exception, the stack starts
unwinding, thereby calling the `BufferedReader`’s destructor.

Next I’d like to discuss RAII in GC-managed contexts.

## Hello GC

Note also that RAII is not necessarily a concept tied with non-GC-managed
languages.
Sure, there’s no need to worry about the heap.
Still, GC-managed languages have to take care of releasing files or DB
connections.
That’s why Eclipse makes me `close()` my file.
So GC-managed languages shouldn’t avoid the question of RAII.
And that is why C# came up with the now prevalent `using` statement (I heard
Java 7 eventually came up with a similar thing called try-with-resources
statements).

```C#
class Foo : IDisposable {
  // blah
}

// a couple thousand lines later...
using(Foo f = new Foo()) {
  // blah
}
```

Any class that implements `IDisposable` is eligible to be a using-scoped
resource IIRC.
From the perspective of forgetfulness, these `using` statements are nice because
they forcefully reserve resource release at initialization.
Such measures against forgetfulness is particularly a theme I wish to discuss
throughout the series.

Of course, `using` statements are not perfect RAII.
Real RAII shouldn’t even need a `using` block.
But at least the C# guys tried!
They deserve credit.

So we know how RAII has settled into C++, C# and Java.
What about C?
As far as I know, C doesn’t have enough syntax to bring such a concept around.

## A Quick Disclaimer about GC

I just want to quickly clarify my position that I’m not implying GC is bad.
I haven’t stated any opinions against heap memory not having RAII in Java or C#.
GC has pros and cons, some of which have to do with performance, and I do not
wish to take side.
I feel it’s reached somewhat of a religious status (I’m looking forward to
Bjarne’s sequel to his [new series](https://isocpp.org/blog/2014/12/myths-1)).

Next I discuss how and when RAII can be forced.

## Force

Now, the languages with RAII we’ve discussed so far, thank you, but — you don’t
force the programmer to utilize RAII.
Enough syntax support is good, but can we go even further?
In C++, you have to use a special smart pointer wrapper around the usual
`delete` operator to implement block-scoped resource release.
Thanks to STL or other libraries, such smart pointers are easy to obtain, but
it’s theoretically optional.
There’s still that room for the code reviewer to yell at the rookie and have him
rewrite the entire code with a smart pointer.

So can we coerce?
If RAII against heap memory is the topic, I’d say: yes, it can be forced.
Let’s check out Rust.

```Rust
struct Point {
    x: f64,
    y: f64,
}

fn foo() {
    {
        let p: Box<Point> = box Point { x: 1.0, y: 2.0 };
    } // p's heap memory is released!
}
```

Rust has a thing called the `box` operator, which in theory is identical to many
C++ smart pointer implementations.
And there is no `delete` operator in Rust.
There just isn’t one, because Rust is a language designed with RAII in mind to
begin with.
The heap memory associated with the variable `p`, which is of type `Box<Point>`,
gets automatically released as `p` goes out of the block scope.

I was browsing librustc’s source code the other day, and was delighted to come
across [this
comment](https://github.com/rust-lang/rust/blob/ba4081a5a8573875fed17545846f6f6902c8ba8d/src/librustc/driver/driver.rs#L64):

```Rust
// We need nested scopes here, because the intermediate results can keep
// large chunks of memory alive and we want to free them as soon as
// possible to keep the peak memory usage low
let (outputs, trans, sess) = {
    let (outputs, expanded_crate, id) = {
        let krate = phase_1_parse_input(&sess, cfg, input);
        if stop_after_phase_1(&sess) { return; }
        let outputs = build_output_filenames(input,
                                             outdir,
                                             output,
                                             krate.attrs.as_slice(),
                                             &sess);
        let id = link::find_crate_name(Some(&sess), krate.attrs.as_slice(),
                                       input);
        let expanded_crate
            = match phase_2_configure_and_expand(&sess, krate, id.as_slice(),
                                                 addl_plugins) {
                None => return,
                Some(k) => k
            };

        (outputs, expanded_crate, id)
    };
    
    // blah...
}
```

This is some RAII in action!
With a nice comment!
I’m particulary happy here because memory release was handled in a syntactical
i.e. forceful way.
Again, I’m enjoying the philosophy behind all this: block scopes equal resource
lifetime.
*That* is RAII.
With RAII, block scoping becomes an isomorphic abstraction of resource lifetime.

## Forced Extra-process RAII - Is It Possible

So that was about heap memory, or more generally maybe, intra-process resources.
What if we’re talking about non-heap resources involving external processes?
Well, my intuition tells me that perfect RAII cannot be pursued in this realm —
because external resources require APIs that can’t be completely predicted at
the time of language design.
But in Rust’s defense, the language does implement many fundamental structs in
RAII style, such as `Vec`, `String`, `File` and `Process` (check out the `Drop`
trait if you’re interested in RAII with your custom `struct` in Rust).

Actually, I’m not completely sure if it’s really theoretically impossible (from
a language perspective) to force RAII against I/O, though.
That’d be an interesting topic I know nothing of yet.

Lastly, I discuss two cases where RAII can go wrong.

## Ownership / Lifetime

Without diving into this vast topic on my own, let me borrow an example from
[Eric Kidd](http://www.randomhacks.net/2014/09/19/rust-lifetimes-reckless-cxx/):

```C++
vector<pair<const char *,const char *>> tokenize_string2(const string &text);
auto v = tokenize_string2(get_input_string()); // Disaster strikes!
munge(v);
```

Why does disaster strike in this code?
Let me quote the same Eric.

>>The function get_input_string returns a temporary string, and
>>tokenize_string2builds an array of pointers into that string.
>>Unfortunately, the temporary string only lives until the end of the current
>>expression, and then the underlying memory is released.
>>And so all our pointers in v now point into oblivion — and our program just
>>wound up getting featured in a CERT advisory.

So yeah, that intuitively written piece of code (I know it’s not the best way to
write that thing in C++, I know) causes an error because the writer forgot about
how long the `char*` pointers are owned by whom.
If I were to really pursue RAII, I shall watch out for errors like that.
If I were in a situation to find such caution too burdensome, I would just
resort to a GC-managed language, like JavaScript.
I believe this is one of the reasons why I produce code much more quickly in JS.
And JS code isn’t even that slow these days! 

Rust has an exhaustive ownership checker guaranteeing that such ownership errors
do not occur.
I will stop here about Rust.
You should try Rust if you want to prevent errors like the above at a language
level (which, again, is a theme of this series).

## C++11: the `noexcept` keyword

My last section is about a keyword added to C++11.
Earlier I commended the C++ style of releasing resources within destructors.
But what if such a release, which usually involves I/O with external processes,
raises an exception?
What if the OS declines to accept that file resource you’re trying to return
(this might be a silly example, but who knows)?
The probability of getting an error during I/O is relatively high, I’d say.
Gosh I/O, why are you so difficult.

Remember the old rule of thumb in C++?
“Do not throw exceptions in the destructor.”
This rule is also important in the RAII sense.
Say some OS exception occurred in a destructor, and then the error stack starts
backtracing.
What if somewhere up the destructor chain there is another file resource to be
released, a DB connection to be freed, etc.?
Because the exception is chasing down the other error trace, there’s a good
chance these other resources are being missed away from getting closed.
Not good.
This is RAII going wrong.

That is why the programmer should take particular caution when dealing with
exception-raising calls.
Implement a **Plan B** in case things go wrong — a default action in case of an
exception.

Yet like I said earlier, people forget.
So what language device is in order?
After all, I’m supposed to be writing about programming languages.
We need something that puts the “no exceptions whatsoever, really please” label,
at least on destructors.
But this feels like an idealistic pursuit for a language as immense and complex
as C++; who knows what some random method deep down at the 1000th line of the
stack trace (1000 lines of stack trace easily seems like a feasible thing in
C++) would throw?

C++11's newly added `noexcept` keyword, to me, feels like a compromise between
this idealism and the reality.
When an exception occurs inside a method marked `noexcept`, the program will
stop immediately by calling `std::terminate()` which terminates the process with
a status code.
I’d say this is reasonable behavior — when an absolutely unexpected thing
happens, it’s nice to freak out ASAP (i.e. terminating the process) and give the
tester an earlier view into the issue at hand.
Instead of elaborating further on this topic myself, I’d like to quote [Andrzej
Krzemieński](https://akrzemi1.wordpress.com/2014/04/24/noexcept-what-for/).

>>If an exception is thrown from a destructor during stack unwinding
>>std::terminate is called.
>>If this is your design decision that your destructors should throw, calling
>>std::terminate is an acceptable and desired consequence.
>>On the other hand, if you are trying to apply the best practice widely
>>accepted in C++ community that destructors should not throw, calling
>>std::terminate in this case is an indication that you inadvertently did throw
>>from a destructor.
>>In that case it is a bug, but one that occurs very seldom.
>>Suppose that you throw from a destructor 1 in 1000 times.
>>Even if you do, it is again 1 in a 1000 times that your exception is called
>>due to stack unwinding.
>>So the probability of calling std::terminate due to double-exception situation
>>is 1/1000000 (assuming the two events are independent), and is likely not to
>>occur during testing, and only in production.
>>On the other hand, if the destructor is marked as noexcept, std::terminate is
>>going to be called whenever an exception is thrown from destructor, during
>>stack unwinding or normal scope exit.
>>So the probability of the throw from destructor being revealed is 1/1000
>>instead of 1/000000, and it becomes more likely that we will see it when the
>>program is still in testing stage.
>>And when it happens during testing, the provision that the stack need not be
>>unwound proves very beneficial, because std::terminate can be called directly
>>at the point of the throw and we can precisely identify the culprit.
>>
>>For this reason destructors are implicitly declared noexcept in C++11.
>>Is this a convincing use case? Answer yourself.

I could go on for a while about how languages should handle exceptions, etc.,
but I’ve written enough for today.

## Conclusion

-   RAII is nice if I can get ownership and destructor exceptions figured out.

-   In case where taking care of all this costs me too much time, I wouldn’t
mind using a garbage collector (analogous to why I said I wouldn’t mind using a
dynamically typed language in Part 1).

-   But even in a GC-managed language, if the IDE is going to yell at me for not
calling `file.close()`, having a syntactical device like the `using()` clause to
remind the programmer may not be bad.

-   On the other hand, if I do decide to stick with RAII, it’d be nice if the
language forced me to do so.
Like how Rust only has the `box` operator without a `delete`.

-   Again, ownership and destructor exceptions can get tricky.
Depending on the aimed level of reliability, I shall remind myself every once in
a while.

Pending Question

-   Is it theoretically possible to force RAII against I/O at the language
level?
