# Reading notes: GPU text rendering with vector textures

Link: http://wdobbie.com/post/gpu-text-rendering-with-vector-textures/

This write-up investigates how to render font glyphs for use in GL.

## Summary

Apparently, a lot of people just render a lot of the characters to be used at
all possible sizes at once, on the CPU.
This is called a *font atlas*.
As anyone can imagine, this may not be the smartest thing to do:
- Uses the CPU, putting more constraints on overall computation of the program
and on the rendition efficiency (GPUs can do a lot more things at once)
- The glyphs don't scale infinitely, because they're not vectors
- etc etc

There's another technique called signed distance fields too.
I have no idea how it works, but apparently it has the problem of sharp corners
being rounded out at higher scales, though this method does scale infinitely.

So let's go back to the basics and think.
Glyphs are just a bunch of bezier curves...
So why not just render these curves directly on the GPU, without going through
the CPU?

When we go down this route, since GPUs are good at parallelization, we need to
think about how to maximize the rendering efficiency.
Naturally, we can think in terms of the pixels forming a grid, and the bezier
curves going through each grid cell.
From this perspective, we can write a shader in which every pixel only cares
about how much of its area intersects with the glyph.
Then, if, say, 40% of its area is covered, we set the pixel to `rgba(0, 0, 0,
0.4)`.
Clever.

How do we compute this area ratio?
Using Green's Theorem (ah, good old vector cal) is one way to do it, but it gets
tricky apparently (I'm not sure how tricky though).
So we reduce the problem to one-dimesnion.
We shoot a ray from the left end of the circle to the right end.
Every time the ray enters the glyph, we're getting more area.
Every time it exits, we're getting less area.
This way, we can approximate the ratio of area coverage.
Finding these intersections comes down to solving quadratic equations (because
the curves are quadratic Bezier).

But in certain situations (e.g. curves at glancing angles), the approximation
may be way off.
So we shoot the rays at different angles (aka *supersampling*).

Since this approach is completely rendered in the GPU, we can get infinite
scalability and rotate, shift by a subpixel amount, etc.!

## Thoughts

- Nice write-up and execution.
Respect!

## Lingering questions

- I still don't really understand that atlas looking like a download error.
Why is there still an atlas with this approach?
And why does it look like a download error?

## Further research to do:

- Let's read the implementation notes.
- Apparently there are two kinds of Bezier curves: quadratic and cubic.
Quadratic ones are parametrized by three parameters; cubic, four.
And choosing either results in different computation implications throughout the
process.
Interesting, I'll have to learn more about Bezier curves!
- Why should each pixel be treated as a circle?
I'll have to read http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf
- What is a window function?
- Why do we "want to use an arbitrary window function for better quality"?
And why does that want make using Green's Theorem "a bit tricky to implement in
a shader"?
Maybe it's all explained in the Manson and Schaefer paper.
- How is antialiasing done in other implementations?
